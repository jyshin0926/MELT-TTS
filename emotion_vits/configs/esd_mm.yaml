
train:
  log_interval: 200
  eval_interval: 1000
  seed: 1234
  epochs: 10000
  learning_rate: 2e-4
  betas: [0.8, 0.99]
  eps: 1e-9
  batch_size: 4
  fp16_run: true
  lr_decay: 0.999875
  segment_size: 8192
  init_lr_ratio: 1
  warmup_epochs: 0
  c_mel:
  c_kl:


data:
  training_files:
    speaker:
    emotion:
    datadir:
    audiodir:
    filelist:

  validation_files:
    speaker:
    emotion:
    datadir:
    audiodir:
    filelist:
  
  text_cleaners:
  max_wav_value:
  sampling_rate:
  filter_length:
  hop_length:
  win_length:
  n_mel_channels:
  mel_fmin:
  mel_fmax:
  add_blank:
  n_speakers:
  n_emotions:
  n_sensitivity:
  # cleaned_text

model:
  inter_channels: 192
  hidden_channels: 192
  filter_channels: 768
  n_heads: 2
  n_layers: 6
  kernel_size: 3
  p_dropout: 0.1

  resblock_kernel_sizes: [3,7,11]
  resblock_dilation_sizes: [[1,3,5],[1,3,5],[1,3,5]]
  upsample_rates: [8,8,2,2]
  upsample_initial_channel: 512
  upsample_kernel_sizes: [16, 16, 4, 4]
  n_layers_q: 3
  use_spectral_norm: false
  gin_channels: 256
  vision_model_path: /workspace/jaeyoung/checkpoints/onepeace/mmtts_vl_1013/checkpoint_last.pt
  audio_model_path: /workspace/jaeyoung/checkpoints/onepeace/esd_mmtts_al_1014/checkpoint_last.pt
  emotion_classes:


